{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def fetch_github_issues(owner=\"langflow-ai\", repo=\"langflow\",token=None):\n",
    "    \"\"\"\n",
    "    Fetch issues from a GitHub repository using requests\n",
    "    :param owner: Repository owner/organization\n",
    "    :param repo: Repository name\n",
    "    \"\"\"\n",
    "    # GitHub API URL for issues\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/issues\"\n",
    "    \n",
    "    # Add headers to avoid rate limiting issues\n",
    "    headers = {\n",
    "        'Accept': 'application/vnd.github.v3+json',\n",
    "        'User-Agent': 'Python-GitHub-Issues-Example'\n",
    "    }\n",
    "    if github_token:\n",
    "        headers['Authorization'] = f'token {github_token}'\n",
    "    \n",
    "    try:\n",
    "        # Make the GET request\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            issues = response.json()\n",
    "            \n",
    "            print(f\"\\nFetching issues from {owner}/{repo}:\")\n",
    "            print(\"-\" * 50)\n",
    "            docs = []\n",
    "            # Print each issue\n",
    "            for issue in issues:\n",
    "                metadata = {\n",
    "                    \"author\": issue[\"user\"][\"login\"],\n",
    "                    \"comments\": issue[\"comments\"],\n",
    "                    \"body\": issue[\"body\"],\n",
    "                    \"labels\": issue[\"labels\"],\n",
    "                    \"created_at\": issue[\"created_at\"],\n",
    "                    }\n",
    "                data = issue[\"title\"]\n",
    "                if issue[\"body\"]:\n",
    "                    data += issue[\"body\"]\n",
    "                doc = Document(page_content=data, metadata=metadata)\n",
    "                docs.append(doc)\n",
    "                \n",
    "        else:\n",
    "            print(f\"Failed to fetch issues. Status code: {response.status_code}\")\n",
    "            print(f\"Error message: {response.text}\")\n",
    "        \n",
    "        return docs\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_vector_db():\n",
    "    try:\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "        \n",
    "        # Get connection string\n",
    "        CONNECTION_STRING = os.getenv('DATABASE_URL')\n",
    "        if not CONNECTION_STRING:\n",
    "            raise ValueError(\"DATABASE_URL environment variable not found\")\n",
    "            \n",
    "        # Initialize embeddings\n",
    "        ollama_embeddings = OllamaEmbeddings(\n",
    "            model=\"mxbai-embed-large:latest\",\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "        COLLECTION_NAME = 'github_issues'  # Fixed typo in collection name\n",
    "        \n",
    "        # Create vector store\n",
    "        vector_store = PGVector(\n",
    "            connection_string=CONNECTION_STRING,\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            embedding_function=ollama_embeddings\n",
    "        )\n",
    "        \n",
    "        return vector_store\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to vector database: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_vector_db(githubIssuesData):\n",
    "\n",
    "    ollama_embeddings = OllamaEmbeddings(\n",
    "        model=\"llama2-uncensored:latest\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    CONNECTION_STRING = os.getenv('DATABASE_URL')\n",
    "    COLLECTION_NAME = 'github_issues'\n",
    "    vector_store = PGVector.from_documents(\n",
    "    embedding=OllamaEmbeddings(model=\"mxbai-embed-large\"),\n",
    "    documents=githubIssuesData,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING)\n",
    "\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saigi\\AppData\\Local\\Temp\\ipykernel_20412\\1659217320.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  ollama_embeddings = OllamaEmbeddings(\n",
      "C:\\Users\\saigi\\AppData\\Local\\Temp\\ipykernel_20412\\1659217320.py:20: LangChainPendingDeprecationWarning: This class is pending deprecation and may be removed in a future version. You can swap to using the `PGVector` implementation in `langchain_postgres`. Please read the guidelines in the doc-string of this class to follow prior to migrating as there are some differences between the implementations. See <https://github.com/langchain-ai/langchain-postgres> for details about the new implementation.\n",
      "  vector_store = PGVector(\n",
      "C:\\Users\\saigi\\AppData\\Local\\Temp\\ipykernel_20412\\1659217320.py:20: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  vector_store = PGVector(\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "github_token = os.getenv('GITHUB_TOKEN')\n",
    "# Fetch issues\n",
    "\n",
    "\n",
    "# Connect to vector database\n",
    "vector_store = connect_to_vector_db()\n",
    "\n",
    "\n",
    "add_to_vectorstore = input(\"Do you want to update the issues? (y/N): \").lower() in [\n",
    "    \"yes\",\n",
    "    \"y\",\n",
    "]\n",
    "\n",
    "if add_to_vectorstore:\n",
    "    owner=\"langflow-ai\"\n",
    "    repo=\"langflow\"\n",
    "    vector_store.delete_collection()\n",
    "    issuesData = fetch_github_issues(owner=owner, repo=repo,token=github_token)\n",
    "    # Add issues to vector database \n",
    "    vector_store = load_data_to_vector_db(issuesData)\n",
    "else:\n",
    "    vector_store = connect_to_vector_db()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_similarity_search(vector_store, query=\"What are the issues related to Langflow crashing?\", k=2):\n",
    "    try:\n",
    "        if vector_store is None:\n",
    "            raise ValueError(\"Vector store not initialized\")\n",
    "            \n",
    "        # Perform similarity search\n",
    "        results = vector_store.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        print(f\"\\nSearch Query: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for doc, score in results:\n",
    "            print(f\"\\nScore: {score:.4f}\")\n",
    "            print(f\"Content: {doc.page_content[:200]}...\")  # Show first 200 chars\n",
    "            print(f\"Metadata: Created at {doc.metadata.get('created_at', 'N/A')}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during similarity search: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Query: What are the issues related to Langflow crashing?\n",
      "--------------------------------------------------\n",
      "\n",
      "Score: 0.2305\n",
      "Content: LangFlow suddenly crashed and can no longer be started### Bug Description\n",
      "\n",
      "Yesterday, I started LangFlow inside a container and created two Vector Store RAG workflows:\n",
      "flow_1: Vectorized Markdown d...\n",
      "Metadata: Created at 2024-11-20T03:39:20Z\n",
      "--------------------------------------------------\n",
      "\n",
      "Score: 0.2850\n",
      "Content: After upgrade to version 1.1.0, the old flows fail to build because components are out of date### Bug Description\n",
      "\n",
      "The problem here is that quite a few components have been upgraded in the new version...\n",
      "Metadata: Created at 2024-11-20T15:30:38Z\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'author': 'tzengshinfu', 'comments': 3, 'body': \"### Bug Description\\r\\n\\r\\nYesterday, I started LangFlow inside a container and created two Vector Store RAG workflows:\\r\\nflow_1: Vectorized Markdown documents and wrote them into PGVector.\\r\\nflow_2: Matched user queries with PGVector to retrieve answers.\\r\\nBoth workflows ran smoothly yesterday.\\r\\n\\r\\nThis morning, I discovered that LangFlow had suddenly crashed, and the log is as follows:\\r\\n[langflow-crash.log](https://github.com/user-attachments/files/17824375/langflow-crash.log)(As seen at [11/20/24 09:39:20])\\r\\n\\r\\nEven after deleting the vectorization records in PGVector, it still couldn't start successfully.\\r\\nUltimately, I had to delete all the tables storing the vectorized document data to get it to start properly.\\r\\n\\r\\n### Reproduction\\r\\n\\r\\n1. Start LangFlow and PGVector (container settings are as shown in the attachment: [docker-compose.yml.txt](https://github.com/user-attachments/files/17824361/docker-compose.yml.txt)).\\r\\n2. Execute [flow_1](https://github.com/user-attachments/files/17824294/flow_1.json) to vectorize the file: [2024-11-19_07-29-59_20221117_edited.md](https://github.com/user-attachments/files/17824322/2024-11-19_07-29-59_20221117_edited.md)\\r\\n and write it into PGVector.\\r\\n3. Execute [flow_2](https://github.com/user-attachments/files/17824301/flow_2.json) to ask a question; everything worked fine at this stage.\\r\\n4. After a while, LangFlow crashed on its own and could not be restarted properly.\\r\\n5. The memory usage of the host at the time was not significantly increasing but rather decreasing (as shown in the attachment: [Memory_usage](https://github.com/user-attachments/assets/7ca1f9cc-9b6a-471f-8d02-01ee44f15ada))\\r\\n\\r\\n### Expected behavior\\r\\n\\r\\nContinues to run normally.\\r\\n\\r\\n### Who can help?\\r\\n\\r\\n_No response_\\r\\n\\r\\n### Operating System\\r\\n\\r\\nUbuntu 22.04.5\\r\\n\\r\\n### Langflow Version\\r\\n\\r\\n1.0.19 (docker)\\r\\n\\r\\n### Python Version\\r\\n\\r\\nNone\\r\\n\\r\\n### Screenshot\\r\\n\\r\\n![image_20241120_103531](https://github.com/user-attachments/assets/71a5bd8a-ae18-4276-b121-4953144156ea)\\r\\n\\r\\n\\r\\n### Flow File\\r\\n\\r\\n[flow_1.json](https://github.com/user-attachments/files/17824294/flow_1.json)\\r\\n[flow_2.json](https://github.com/user-attachments/files/17824301/flow_2.json)\\r\\n[2024-11-19_07-29-59_20221117_edited.md](https://github.com/user-attachments/files/17824322/2024-11-19_07-29-59_20221117_edited.md)\\r\\n[docker-compose.yml.txt](https://github.com/user-attachments/files/17824361/docker-compose.yml.txt)\\r\\n[langflow-crash.log](https://github.com/user-attachments/files/17824375/langflow-crash.log)\\r\\n[Memory_usage](https://github.com/user-attachments/assets/7ca1f9cc-9b6a-471f-8d02-01ee44f15ada)\\r\\n\\r\\n\", 'labels': [{'id': 5132979269, 'node_id': 'LA_kwDOI7jmA88AAAABMfMMRQ', 'url': 'https://api.github.com/repos/langflow-ai/langflow/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': \"Something isn't working\"}], 'created_at': '2024-11-20T03:39:20Z'}, page_content=\"LangFlow suddenly crashed and can no longer be started### Bug Description\\r\\n\\r\\nYesterday, I started LangFlow inside a container and created two Vector Store RAG workflows:\\r\\nflow_1: Vectorized Markdown documents and wrote them into PGVector.\\r\\nflow_2: Matched user queries with PGVector to retrieve answers.\\r\\nBoth workflows ran smoothly yesterday.\\r\\n\\r\\nThis morning, I discovered that LangFlow had suddenly crashed, and the log is as follows:\\r\\n[langflow-crash.log](https://github.com/user-attachments/files/17824375/langflow-crash.log)(As seen at [11/20/24 09:39:20])\\r\\n\\r\\nEven after deleting the vectorization records in PGVector, it still couldn't start successfully.\\r\\nUltimately, I had to delete all the tables storing the vectorized document data to get it to start properly.\\r\\n\\r\\n### Reproduction\\r\\n\\r\\n1. Start LangFlow and PGVector (container settings are as shown in the attachment: [docker-compose.yml.txt](https://github.com/user-attachments/files/17824361/docker-compose.yml.txt)).\\r\\n2. Execute [flow_1](https://github.com/user-attachments/files/17824294/flow_1.json) to vectorize the file: [2024-11-19_07-29-59_20221117_edited.md](https://github.com/user-attachments/files/17824322/2024-11-19_07-29-59_20221117_edited.md)\\r\\n and write it into PGVector.\\r\\n3. Execute [flow_2](https://github.com/user-attachments/files/17824301/flow_2.json) to ask a question; everything worked fine at this stage.\\r\\n4. After a while, LangFlow crashed on its own and could not be restarted properly.\\r\\n5. The memory usage of the host at the time was not significantly increasing but rather decreasing (as shown in the attachment: [Memory_usage](https://github.com/user-attachments/assets/7ca1f9cc-9b6a-471f-8d02-01ee44f15ada))\\r\\n\\r\\n### Expected behavior\\r\\n\\r\\nContinues to run normally.\\r\\n\\r\\n### Who can help?\\r\\n\\r\\n_No response_\\r\\n\\r\\n### Operating System\\r\\n\\r\\nUbuntu 22.04.5\\r\\n\\r\\n### Langflow Version\\r\\n\\r\\n1.0.19 (docker)\\r\\n\\r\\n### Python Version\\r\\n\\r\\nNone\\r\\n\\r\\n### Screenshot\\r\\n\\r\\n![image_20241120_103531](https://github.com/user-attachments/assets/71a5bd8a-ae18-4276-b121-4953144156ea)\\r\\n\\r\\n\\r\\n### Flow File\\r\\n\\r\\n[flow_1.json](https://github.com/user-attachments/files/17824294/flow_1.json)\\r\\n[flow_2.json](https://github.com/user-attachments/files/17824301/flow_2.json)\\r\\n[2024-11-19_07-29-59_20221117_edited.md](https://github.com/user-attachments/files/17824322/2024-11-19_07-29-59_20221117_edited.md)\\r\\n[docker-compose.yml.txt](https://github.com/user-attachments/files/17824361/docker-compose.yml.txt)\\r\\n[langflow-crash.log](https://github.com/user-attachments/files/17824375/langflow-crash.log)\\r\\n[Memory_usage](https://github.com/user-attachments/assets/7ca1f9cc-9b6a-471f-8d02-01ee44f15ada)\\r\\n\\r\\n\"),\n",
       "  0.23047217423915245),\n",
       " (Document(metadata={'author': 'fanimali', 'comments': 0, 'body': \"### Bug Description\\n\\nThe problem here is that quite a few components have been upgraded in the new version, but there isn't a way to simply upgrade the old components. \\r\\nThe old flows will not build anymore and the user must delete and recreate manually those components that are failing the build, one by one.\\r\\n\\r\\nPossible enhancement would be to handle the upgrades in a better way for a neater customer experience.\\n\\n### Reproduction\\n\\nupgrade LF to version 1.1\\r\\nverify that old components are not building anymore and need to be upgraded to new version of the same components.\\n\\n### Expected behavior\\n\\na version upgrade should be backward compatible or the user should be presented with a question as to automatically upgrade the components in the existing flows.\\n\\n### Who can help?\\n\\n_No response_\\n\\n### Operating System\\n\\nUbuntu 22.04\\n\\n### Langflow Version\\n\\n1.1.0\\n\\n### Python Version\\n\\nNone\\n\\n### Screenshot\\n\\n_No response_\\n\\n### Flow File\\n\\n_No response_\", 'labels': [{'id': 5132979269, 'node_id': 'LA_kwDOI7jmA88AAAABMfMMRQ', 'url': 'https://api.github.com/repos/langflow-ai/langflow/labels/bug', 'name': 'bug', 'color': 'd73a4a', 'default': True, 'description': \"Something isn't working\"}], 'created_at': '2024-11-20T15:30:38Z'}, page_content=\"After upgrade to version 1.1.0, the old flows fail to build because components are out of date### Bug Description\\n\\nThe problem here is that quite a few components have been upgraded in the new version, but there isn't a way to simply upgrade the old components. \\r\\nThe old flows will not build anymore and the user must delete and recreate manually those components that are failing the build, one by one.\\r\\n\\r\\nPossible enhancement would be to handle the upgrades in a better way for a neater customer experience.\\n\\n### Reproduction\\n\\nupgrade LF to version 1.1\\r\\nverify that old components are not building anymore and need to be upgraded to new version of the same components.\\n\\n### Expected behavior\\n\\na version upgrade should be backward compatible or the user should be presented with a question as to automatically upgrade the components in the existing flows.\\n\\n### Who can help?\\n\\n_No response_\\n\\n### Operating System\\n\\nUbuntu 22.04\\n\\n### Langflow Version\\n\\n1.1.0\\n\\n### Python Version\\n\\nNone\\n\\n### Screenshot\\n\\n_No response_\\n\\n### Flow File\\n\\n_No response_\"),\n",
       "  0.28503964136493687)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_similarity_search(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saigi\\AppData\\Local\\Temp\\ipykernel_20412\\2579645501.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llama3_llm=Ollama(model=\"llama3.1:latest\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3.1:latest')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "## Load Ollama llam3 LLM model\n",
    "llama3_llm=Ollama(model=\"llama3.1:latest\")\n",
    "llama3_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"github_search\",\n",
    "    \"Search for information about github issues. For any questions about github issues, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def note_tool(note):\n",
    "    \"\"\"\n",
    "    saves a note to a local file\n",
    "\n",
    "    Args:\n",
    "        note: the text note to save\n",
    "    \"\"\"\n",
    "    with open(\"notes.txt\", \"a\") as f:\n",
    "        f.write(note + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find information about GitHub issues related to LiteLLM Connector.\n",
      "Action: github_search\n",
      "Action Input: \"LiteLLM Connector\"\u001b[0m\u001b[36;1m\u001b[1;3mAdd LiteLLM Connector### Feature Request\n",
      "\n",
      "I would like to suggest adding a connector for LiteLLM to LangFlow.\n",
      "LiteLLM is a lightweight and fast open-source library for integrating various LLM providers with a simplified interface. Including this connector in LangFlow would offer several benefits:\n",
      "\n",
      "Broader compatibility: Support for multiple LLM providers through LiteLLM's unified interface.\n",
      "\n",
      "Improved usability: Simplifies the process of integrating and switching between LLMs.\n",
      "\n",
      "Community adoption: LiteLLM has an active and growing user base, making it a valuable addition to LangFlow.\n",
      "\n",
      "\n",
      "Proposed Solution\n",
      "\n",
      "1. Create a new connector module in LangFlow that interfaces with LiteLLM.\n",
      "2. Ensure compatibility with key LiteLLM features, such as provider switching and request optimization.\n",
      "3. Provide documentation and examples for users to set up and use the LiteLLM connector.\n",
      "\n",
      "This feature would make LangFlow more versatile and appealing to users seeking streamlined LLM integration\n",
      "\n",
      "### Motivation\n",
      "\n",
      "This enhancement aligns with LangFlow's goal of being a versatile platform for LLM integration. The inclusion of LiteLLM would:\n",
      "\n",
      "Solve the challenge of managing multiple LLM providers with a unified interface.\n",
      "\n",
      "Reduce friction for users who often switch between providers or experiment with different LLMs.\n",
      "\n",
      "Leverage LiteLLM’s active community for broader adoption and support.\n",
      "\n",
      "\n",
      "Thank you for considering this proposal!\n",
      "\n",
      "### Your Contribution\n",
      "\n",
      "I could test asap this component and share a basic flow.\n",
      "\n",
      "Expanding Unstructured loader to take server file inputs, more file types, and API parametersDepends on  https://github.com/langflow-ai/langflow/pull/4731/files\n",
      "\n",
      "After upgrade to version 1.1.0, the old flows fail to build because components are out of date### Bug Description\n",
      "\n",
      "The problem here is that quite a few components have been upgraded in the new version, but there isn't a way to simply upgrade the old components. \n",
      "The old flows will not build anymore and the user must delete and recreate manually those components that are failing the build, one by one.\n",
      "\n",
      "Possible enhancement would be to handle the upgrades in a better way for a neater customer experience.\n",
      "\n",
      "### Reproduction\n",
      "\n",
      "upgrade LF to version 1.1\n",
      "verify that old components are not building anymore and need to be upgraded to new version of the same components.\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "a version upgrade should be backward compatible or the user should be presented with a question as to automatically upgrade the components in the existing flows.\n",
      "\n",
      "### Who can help?\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Operating System\n",
      "\n",
      "Ubuntu 22.04\n",
      "\n",
      "### Langflow Version\n",
      "\n",
      "1.1.0\n",
      "\n",
      "### Python Version\n",
      "\n",
      "None\n",
      "\n",
      "### Screenshot\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Flow File\n",
      "\n",
      "_No response_\u001b[0m\u001b[32;1m\u001b[1;3mAction: github_search\n",
      "Action Input: \"LiteLLM Connector GitHub issues\"\u001b[0m\u001b[36;1m\u001b[1;3mAdd LiteLLM Connector### Feature Request\n",
      "\n",
      "I would like to suggest adding a connector for LiteLLM to LangFlow.\n",
      "LiteLLM is a lightweight and fast open-source library for integrating various LLM providers with a simplified interface. Including this connector in LangFlow would offer several benefits:\n",
      "\n",
      "Broader compatibility: Support for multiple LLM providers through LiteLLM's unified interface.\n",
      "\n",
      "Improved usability: Simplifies the process of integrating and switching between LLMs.\n",
      "\n",
      "Community adoption: LiteLLM has an active and growing user base, making it a valuable addition to LangFlow.\n",
      "\n",
      "\n",
      "Proposed Solution\n",
      "\n",
      "1. Create a new connector module in LangFlow that interfaces with LiteLLM.\n",
      "2. Ensure compatibility with key LiteLLM features, such as provider switching and request optimization.\n",
      "3. Provide documentation and examples for users to set up and use the LiteLLM connector.\n",
      "\n",
      "This feature would make LangFlow more versatile and appealing to users seeking streamlined LLM integration\n",
      "\n",
      "### Motivation\n",
      "\n",
      "This enhancement aligns with LangFlow's goal of being a versatile platform for LLM integration. The inclusion of LiteLLM would:\n",
      "\n",
      "Solve the challenge of managing multiple LLM providers with a unified interface.\n",
      "\n",
      "Reduce friction for users who often switch between providers or experiment with different LLMs.\n",
      "\n",
      "Leverage LiteLLM’s active community for broader adoption and support.\n",
      "\n",
      "\n",
      "Thank you for considering this proposal!\n",
      "\n",
      "### Your Contribution\n",
      "\n",
      "I could test asap this component and share a basic flow.\n",
      "\n",
      "After upgrade to version 1.1.0, the old flows fail to build because components are out of date### Bug Description\n",
      "\n",
      "The problem here is that quite a few components have been upgraded in the new version, but there isn't a way to simply upgrade the old components. \n",
      "The old flows will not build anymore and the user must delete and recreate manually those components that are failing the build, one by one.\n",
      "\n",
      "Possible enhancement would be to handle the upgrades in a better way for a neater customer experience.\n",
      "\n",
      "### Reproduction\n",
      "\n",
      "upgrade LF to version 1.1\n",
      "verify that old components are not building anymore and need to be upgraded to new version of the same components.\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "a version upgrade should be backward compatible or the user should be presented with a question as to automatically upgrade the components in the existing flows.\n",
      "\n",
      "### Who can help?\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Operating System\n",
      "\n",
      "Ubuntu 22.04\n",
      "\n",
      "### Langflow Version\n",
      "\n",
      "1.1.0\n",
      "\n",
      "### Python Version\n",
      "\n",
      "None\n",
      "\n",
      "### Screenshot\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Flow File\n",
      "\n",
      "_No response_\n",
      "\n",
      "fix: handle Message in path input in CSVAgentComponentThis pull request introduces a helper method to handle the `path` attribute in the `CSVAgentComponent` class in the `src/backend/base/langflow/components/langchain_utilities/csv.py` file. The main changes include the addition of the `_path` method and its integration into the existing methods for building agents.\n",
      "\n",
      "Key changes:\n",
      "\n",
      "* Added `_path` method to handle `path` attribute conversion from `Message` to `str`. (`src/backend/base/langflow/components/langchain_utilities/csv.py`)\n",
      "* Updated `build_agent_response` method to use the new `_path` method for `path` attribute. (`src/backend/base/langflow/components/langchain_utilities/csv.py`)\n",
      "* Updated `build_agent` method to use the new `_path` method for `path` attribute. (`src/backend/base/langflow/components/langchain_utilities/csv.py`)\u001b[0m\u001b[32;1m\u001b[1;3mAction: github_search\n",
      "Action Input: \"LiteLLM Connector GitHub issues\"\u001b[0m\u001b[36;1m\u001b[1;3mAdd LiteLLM Connector### Feature Request\n",
      "\n",
      "I would like to suggest adding a connector for LiteLLM to LangFlow.\n",
      "LiteLLM is a lightweight and fast open-source library for integrating various LLM providers with a simplified interface. Including this connector in LangFlow would offer several benefits:\n",
      "\n",
      "Broader compatibility: Support for multiple LLM providers through LiteLLM's unified interface.\n",
      "\n",
      "Improved usability: Simplifies the process of integrating and switching between LLMs.\n",
      "\n",
      "Community adoption: LiteLLM has an active and growing user base, making it a valuable addition to LangFlow.\n",
      "\n",
      "\n",
      "Proposed Solution\n",
      "\n",
      "1. Create a new connector module in LangFlow that interfaces with LiteLLM.\n",
      "2. Ensure compatibility with key LiteLLM features, such as provider switching and request optimization.\n",
      "3. Provide documentation and examples for users to set up and use the LiteLLM connector.\n",
      "\n",
      "This feature would make LangFlow more versatile and appealing to users seeking streamlined LLM integration\n",
      "\n",
      "### Motivation\n",
      "\n",
      "This enhancement aligns with LangFlow's goal of being a versatile platform for LLM integration. The inclusion of LiteLLM would:\n",
      "\n",
      "Solve the challenge of managing multiple LLM providers with a unified interface.\n",
      "\n",
      "Reduce friction for users who often switch between providers or experiment with different LLMs.\n",
      "\n",
      "Leverage LiteLLM’s active community for broader adoption and support.\n",
      "\n",
      "\n",
      "Thank you for considering this proposal!\n",
      "\n",
      "### Your Contribution\n",
      "\n",
      "I could test asap this component and share a basic flow.\n",
      "\n",
      "After upgrade to version 1.1.0, the old flows fail to build because components are out of date### Bug Description\n",
      "\n",
      "The problem here is that quite a few components have been upgraded in the new version, but there isn't a way to simply upgrade the old components. \n",
      "The old flows will not build anymore and the user must delete and recreate manually those components that are failing the build, one by one.\n",
      "\n",
      "Possible enhancement would be to handle the upgrades in a better way for a neater customer experience.\n",
      "\n",
      "### Reproduction\n",
      "\n",
      "upgrade LF to version 1.1\n",
      "verify that old components are not building anymore and need to be upgraded to new version of the same components.\n",
      "\n",
      "### Expected behavior\n",
      "\n",
      "a version upgrade should be backward compatible or the user should be presented with a question as to automatically upgrade the components in the existing flows.\n",
      "\n",
      "### Who can help?\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Operating System\n",
      "\n",
      "Ubuntu 22.04\n",
      "\n",
      "### Langflow Version\n",
      "\n",
      "1.1.0\n",
      "\n",
      "### Python Version\n",
      "\n",
      "None\n",
      "\n",
      "### Screenshot\n",
      "\n",
      "_No response_\n",
      "\n",
      "### Flow File\n",
      "\n",
      "_No response_\n",
      "\n",
      "fix: handle Message in path input in CSVAgentComponentThis pull request introduces a helper method to handle the `path` attribute in the `CSVAgentComponent` class in the `src/backend/base/langflow/components/langchain_utilities/csv.py` file. The main changes include the addition of the `_path` method and its integration into the existing methods for building agents.\n",
      "\n",
      "Key changes:\n",
      "\n",
      "* Added `_path` method to handle `path` attribute conversion from `Message` to `str`. (`src/backend/base/langflow/components/langchain_utilities/csv.py`)\n",
      "* Updated `build_agent_response` method to use the new `_path` method for `path` attribute. (`src/backend/base/langflow/components/langchain_utilities/csv.py`)\n",
      "* Updated `build_agent` method to use the new `_path` method for `path` attribute. (`src/backend/base/langflow/components/langchain_utilities/csv.py`)\u001b[0mAn error occurred: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `Based on your input, I was not able to find specific information or answer to your question as it seems to be a collection of text snippets related to GitHub issues and pull requests for the LangFlow project.\n",
      "\n",
      "If you could provide more context or clarify what you are trying to achieve, I'd be happy to try and assist you further.`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "tools = [retriever_tool, note_tool]\n",
    "# Create a prompt template for ReAct agent\n",
    "prompt = PromptTemplate.from_template(\"\"\"You are a helpful AI assistant that helps users find information about GitHub issues.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\")\n",
    "\n",
    "# Create the ReAct agent\n",
    "agent = create_react_agent(llama3_llm, tools, prompt)\n",
    "\n",
    "# Create the executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Interactive loop\n",
    "while (question := input(\"Ask a question about github issues (q to quit): \")) != \"q\":\n",
    "    try:\n",
    "        result = agent_executor.invoke({\"input\": question})\n",
    "        print(result[\"output\"])\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
